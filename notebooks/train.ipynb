{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_data, define_additional_args, compute_hypergeometric, configure_seeds\n",
    "from src.minibatch import Minibatch\n",
    "from src.trainer import Trainer\n",
    "from src.evaluator import Evaluator\n",
    "import torch\n",
    "import numpy as np\n",
    "import pdb\n",
    "from datetime import date, datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './data/ppi'\n",
    "# num_subgraphs = 200\n",
    "# num_par_samplers = 10\n",
    "# use_cuda = True\n",
    "# sampler_args = {\n",
    "#     'method': 'rw',\n",
    "#     'num_root': 200,\n",
    "#     'depth': 1\n",
    "# }\n",
    "# num_iterations = 5000\n",
    "# model_args = {\n",
    "#     'arch': 'GraphSAGE',\n",
    "#     'hidden_channels': 512,\n",
    "#     'dropout': 0.1,\n",
    "#     'num_layers': 1\n",
    "# }\n",
    "# training_args = {\n",
    "#     'method': 'normal',\n",
    "#     'loss': 'sigmoid',\n",
    "#     'lr': 0.01,\n",
    "#     'clip_norm': 5\n",
    "# }\n",
    "# save_model_dir = f'/nfs/students/ayle/NodeDP/models/{str(datetime.now())}'\n",
    "# eval_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/ogbn-arxiv_undirected'\n",
    "num_subgraphs = 200\n",
    "num_par_samplers = 10\n",
    "use_cuda = True\n",
    "sampler_args = {\n",
    "    'method': 'drw',\n",
    "    'num_root': 200,\n",
    "    'depth': 1,\n",
    "    'only_roots': False,\n",
    "    'max_degree': 7\n",
    "}\n",
    "num_iterations = 1000\n",
    "model_args = {\n",
    "    'arch': 'GCN2',\n",
    "    'hidden_channels': 512,\n",
    "    'dropout': 0.1,\n",
    "    'num_layers': 1,\n",
    "    'activation': 'relu'\n",
    "}\n",
    "training_args = {\n",
    "    'method': 'ours',\n",
    "    'distribution': 'hyper',\n",
    "    'loss': 'softmax',\n",
    "    'lr': 0.01,\n",
    "    'optim': 'Adam',\n",
    "    'C%': 0.75,\n",
    "    \n",
    "    'alpha': 2,\n",
    "    'delta': 1e-4\n",
    "}\n",
    "save_model_dir = f'/nfs/students/ayle/NodeDP/models/{str(datetime.now())}'\n",
    "eval_every = 10\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './data/ogbn-arxiv_undirected'\n",
    "# num_subgraphs = 200\n",
    "# num_par_samplers = 10\n",
    "# use_cuda = True\n",
    "# sampler_args = {\n",
    "#     'method': 'nodes_max',\n",
    "#     'num_nodes': 5000,\n",
    "#     'max_degree': 7,\n",
    "#     'only_roots': False,\n",
    "# }\n",
    "# num_iterations = 1000\n",
    "# model_args = {\n",
    "#     'arch': 'GCN2',\n",
    "#     'hidden_channels': 512,\n",
    "#     'dropout': 0.1,\n",
    "#     'num_layers': 1,\n",
    "#     'activation': 'tanh'\n",
    "# }\n",
    "# training_args = {\n",
    "#     'method': 'node_dp_max_degree',\n",
    "#     'distribution': 'hyper',\n",
    "#     'loss': 'softmax',\n",
    "#     'lr': 0.01,\n",
    "#     'optim': 'Adam',\n",
    "#     'C%': 0.75,\n",
    "        \n",
    "#     'alpha': 2,\n",
    "#     'delta': 1e-4\n",
    "# }\n",
    "# save_model_dir = f'/nfs/students/ayle/NodeDP/models/{str(datetime.now())}'\n",
    "# eval_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configure_seeds(seed, 'cuda' if use_cuda else 'cpu')\n",
    "\n",
    "adj_full, adj_train, feats, class_arr, role = load_data(data_path, out)\n",
    "num_subgraphs_per_sampler = define_additional_args(num_subgraphs, num_par_samplers, out)\n",
    "minibatch = Minibatch(adj_full, adj_train, role, num_par_samplers, num_subgraphs_per_sampler, use_cuda,\n",
    "                      sampler_args)\n",
    "trainer = Trainer(training_args, model_args, feats, class_arr, use_cuda, minibatch, out, sampler_args)\n",
    "evaluator = Evaluator(model_args, feats, class_arr, training_args['loss'])\n",
    "\n",
    "if training_args['method'] == 'ours':\n",
    "    total_gamma = 0\n",
    "    C = trainer.C  # max sensitivity\n",
    "\n",
    "    if training_args['distribution'] == 'hyper':\n",
    "        K = (sampler_args['max_degree'] ** (model_args['num_layers'] + 1) - 1) // (sampler_args['max_degree'] - 1)\n",
    "        m = sampler_args['num_root'] * (sampler_args['depth'] + 1)  # number of gradients in one batch\n",
    "        gho = compute_hypergeometric(len(minibatch.node_train), K, m)\n",
    "\n",
    "        if not sampler_args['only_roots']:\n",
    "            sigma_without_C = 2*K\n",
    "            sigma_without_K = 2*C\n",
    "        else:\n",
    "            gho = [gho[0], sum(gho[1:])]\n",
    "            sigma_without_C = 1\n",
    "            sigma_without_K = C\n",
    "\n",
    "    elif training_args['distribution'] == 'ours':\n",
    "        assert sampler_args['only_roots']\n",
    "        gho_1 = sum([(sampler_args['max_degree'] + sampler_args['depth']) / (len(minibatch.node_train) - i*(sampler_args['depth'] + 1)) for i in range(sampler_args['num_root']+1)])\n",
    "        gho = [1-gho_1, gho_1]\n",
    "        sigma_without_C = 1\n",
    "        sigma_without_K = C\n",
    "\n",
    "elif training_args['method'] == 'node_dp_max_degree':\n",
    "    K = (sampler_args['max_degree'] ** (model_args['num_layers'] + 1) - 1) // (sampler_args['max_degree'] - 1)  # number of affected nodes in one batch\n",
    "    m = sampler_args['num_nodes']  # number of nodes sampled in one batch\n",
    "    C = trainer.C  # max sensitivity\n",
    "    sigma_without_C = 2 * K\n",
    "    sigma_without_K = 2 * C\n",
    "\n",
    "    total_gamma = 0\n",
    "    gho = compute_hypergeometric(len(minibatch.node_train), K, m)\n",
    "\n",
    "all_eps = []\n",
    "all_iterations = []\n",
    "all_metrics = []\n",
    "\n",
    "t1 = time.time()\n",
    "for it in range(1, num_iterations+1):\n",
    "    if training_args['method'] == 'normal':\n",
    "        trainer.train_step(*minibatch.sample_one_batch(out))\n",
    "    elif training_args['method'] in ['ours', 'node_dp_max_degree']:\n",
    "        trainer.dp_train_step(*minibatch.sample_one_batch(out), sigma=sigma_without_C)\n",
    "\n",
    "        total_gamma += 1 / (training_args['alpha'] - 1) * np.log(sum(np.array([p * (\n",
    "            np.exp(training_args['alpha'] * (training_args['alpha'] - 1) * (i * sigma_without_K) ** 2 / (2*(sigma_without_C * C) ** 2))) for i, p in enumerate(gho)])))\n",
    "\n",
    "    if it % eval_every == 0:\n",
    "        t2 = time.time()\n",
    "        evaluator.model.load_state_dict(trainer.model.state_dict())\n",
    "        preds, labels = evaluator.eval_step(*minibatch.sample_one_batch(out, mode='val'))\n",
    "        metrics = evaluator.calc_metrics(preds, labels)\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        all_iterations.append(it)\n",
    "\n",
    "        print_statement = f\"Iteration {it}:\"\n",
    "        for metric, val in metrics.items():\n",
    "            print_statement += f\"\\t {metric} = {val}\"\n",
    "        print_statement += f\"\\t Training Time = {t2 - t1}\"\n",
    "        out(print_statement)\n",
    "\n",
    "        if training_args['method'] in ['ours', 'node_dp_max_degree']:\n",
    "            out(\"RDP: (\" + str(training_args['alpha']) + \",\" + str(total_gamma) + \")\")\n",
    "            eps = total_gamma + np.log(1 / training_args['delta']) / (training_args['alpha'] - 1)\n",
    "            out(\"DP: (\" + str(eps) + \",\" + str(training_args['delta']) + \")\")\n",
    "            \n",
    "            all_eps.append(eps)\n",
    "\n",
    "        t1 = time.time()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
